{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "import time\n",
    "import requests\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.exceptions import Timeout\n",
    "from IPython.display import clear_output\n",
    "\n",
    "reddit = praw.Reddit(client_id='2qM5x5EPvG5FUw',\n",
    "                     client_secret='FZPbAzurDBqSvfSxQbAv4S-nxrI',\n",
    "                     user_agent='my user agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataframe():\n",
    "    try:\n",
    "        \n",
    "        # if the csv file already exists, load that\n",
    "        df = pd.read_csv('results.csv')\n",
    "        print('File found')\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        # else, create a fresh one\n",
    "        print('File not found. creating new dataframe...')\n",
    "        data = {'submissionid' : [], 'tile' : [], 'upvotes' : [], 'upvote_ratio' : [], 'comments' : [], \n",
    "                   'top_level_comments' : [], 'crossposts' : [], 'awards' : [], 'time_of_post' : [], \n",
    "                   'time_of_request' : [], 'locked' : [], 'removed' : [], 'reason_removed' : [], 'domain' : [], \n",
    "                   'url' : [], 'uploader' : [], 'subreddit' : [], 'flair' : [], 'fake' : []}\n",
    "        df = pd.DataFrame(data=data)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(s, df):\n",
    "    \n",
    "    # make the object non lazy\n",
    "    temp = s.title\n",
    "    \n",
    "    # check if the post is locked\n",
    "    if s.removal_reason == None:\n",
    "        removed = False\n",
    "    else:\n",
    "        removed = True\n",
    "        \n",
    "    # set the data to put in the dataframe\n",
    "    data = {'submissionid' : [s.id], 'tile' : [temp], 'upvotes' : [s.ups], 'upvote_ratio' : [s.upvote_ratio], \n",
    "            'comments' : [s.num_comments], \n",
    "            'top_level_comments' : [len(s._comments_by_id)], 'crossposts' : [s.num_crossposts], \n",
    "            'awards' : [s.total_awards_received], 'time_of_post' : [str(s.created_utc)],\n",
    "            'time_of_request' : [str(time.time())], 'locked' : [s.locked], 'removed' : [removed], \n",
    "            'reason_removed' : [s.removal_reason], 'domain' : [s.domain], 'url' : [s.url], 'uploader' : [s.author.name], \n",
    "            'subreddit' : [s.subreddit.display_name], 'flair' : [s.link_flair_text], 'fake' : [None]}\n",
    "    \n",
    "    # merge with the existing dataframe \n",
    "    return pd.concat([df, pd.DataFrame(data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(submissions, hours, df):\n",
    "    \n",
    "    # set hours to seconds and set start time\n",
    "    seconds = hours * 3600\n",
    "    start_time = time.time()\n",
    "    loops = 1\n",
    "    \n",
    "    # loop while the run time is less than seconds\n",
    "    while True:\n",
    "        \n",
    "        # get start time of the loop\n",
    "        loop_time = time.time()\n",
    "        posts = 0\n",
    "        \n",
    "        # loop over every post\n",
    "        for s in submissions: \n",
    "            posts += 1\n",
    "            \n",
    "            # check if run time is more than secondes\n",
    "            end_time = time.time()\n",
    "            if end_time - start_time > seconds:\n",
    "                \n",
    "                # print run time, and return df\n",
    "                print(round((end_time - start_time) / 3600,2))\n",
    "                return df\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                # try to get the data\n",
    "                df = get_data(reddit.submission(id=s), df)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        # print loop statistics\n",
    "        t = datetime.datetime.now()\n",
    "        print(f'Time elapsed(loop {loops}, posts {posts}): {round(end_time - loop_time, 2)} seconds.\\n \\\n",
    "        Time since start: {round((end_time - start_time) / 3600,2)} hours at {t.hour}:{t.minute}\\n')\n",
    "        loops += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(url):\n",
    "    try:\n",
    "        \n",
    "        # try to clean\n",
    "        try:\n",
    "            \n",
    "            # try to get the news page in less than 10 seconds\n",
    "            html = requests.get(url, timeout=10).text\n",
    "        except Timeout:\n",
    "            \n",
    "            # esle time out\n",
    "            print('Timeout in clean()')\n",
    "            return ''\n",
    "        \n",
    "    except:\n",
    "        print('Unknown error in clean()')\n",
    "        return ''\n",
    "        \n",
    "    soup = BeautifulSoup(html)\n",
    "\n",
    "    # kill all script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()    # rip it out\n",
    "\n",
    "    # get text\n",
    "    text = soup.get_text()\n",
    "\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    return ' '.join([x for x in text.split('\\n') if len(x.split(' ')) > 3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_fake_news(df, name):\n",
    "    \n",
    "    # set base url, get list of unique posts, set data template\n",
    "    BASE = 'http://localhost:8080/fakebox/check'\n",
    "    posts = list(df[df.fake.isnull()].url.unique())\n",
    "    n_posts = len(posts)\n",
    "    counter = 0\n",
    "    data = {\"url\": '',\n",
    "            \"title\": '',\n",
    "            \"content\": ''}\n",
    "    \n",
    "    # write progress log\n",
    "    with open('Results folder\\\\progress.txt', 'a') as f:\n",
    "        f.write(f'In progress: {name}, to do: {n_posts}\\n')\n",
    "    \n",
    "    # loop over every unique posts\n",
    "    for post in posts:\n",
    "        \n",
    "        # get cleaned content of news article\n",
    "        content = clean(post)\n",
    "        if content == '':\n",
    "            \n",
    "            # if no content is returned, call fail()\n",
    "            df, counter = fail(post, df, counter, n_posts)\n",
    "            continue\n",
    "        \n",
    "        # set data atributes\n",
    "        data['content'] = content\n",
    "        data['url'] = post\n",
    "        \n",
    "        # make sure response from api is in json\n",
    "        try:\n",
    "            try:\n",
    "                \n",
    "                # try to get the api output\n",
    "                response = requests.post(BASE, data=data, timeout=60).json()\n",
    "            except Timeout:\n",
    "                \n",
    "                # if it times out, notify users and run fail()\n",
    "                print('error: TIMEOUT' , post)\n",
    "                df, counter = fail(post, df, counter, n_posts)\n",
    "                continue\n",
    "                \n",
    "        except:\n",
    "            \n",
    "            # if respons not in json, notify user and run fail()\n",
    "            print('error: JSON' , post)\n",
    "            df, counter = fail(post, df, counter, n_posts)\n",
    "            continue\n",
    "        \n",
    "        # run succes to add api resonse to dataframe\n",
    "        df, counter = succes(post, df, counter, n_posts, response)\n",
    "    \n",
    "    # update progress losg\n",
    "    with open('Results folder\\\\progress.txt', 'a') as f:\n",
    "        f.write(f'{name} done\\n\\n')\n",
    "    \n",
    "    # return df\n",
    "    return df\n",
    "\n",
    "# is run when something in the above process fails\n",
    "def fail(post, df, counter, n):\n",
    "    \n",
    "    # print where the error occurs\n",
    "    print(counter, ' of ', n, ' fail')\n",
    "    counter += 1\n",
    "    \n",
    "    # update dataframe with non in the fake column\n",
    "    df.loc[df.url == post, 'fake'] = None\n",
    "    return df, counter\n",
    "\n",
    "# is run when everything in tag_fake_news() works\n",
    "def succes(post, df, counter, n, response):\n",
    "    \n",
    "    # try to add api response to the dataframe\n",
    "    try:\n",
    "        \n",
    "        # if response is unsure or biased, set fake news to true\n",
    "        if response['content']['decision'] != 'impartial':\n",
    "            df.loc[df.url == post, 'fake'] = True\n",
    "        \n",
    "        # else set to false\n",
    "        else:\n",
    "            df.loc[df.url == post, 'fake'] = False\n",
    "        \n",
    "        # update the user\n",
    "        print(counter, ' of ', n, ' succes')\n",
    "        counter += 1\n",
    "        \n",
    "    # if something goes wrong, run fail() anyway\n",
    "    except:\n",
    "        df, counter = fail(post, df, counter, n)\n",
    "        print('No response')\n",
    "    \n",
    "    # return counter and df\n",
    "    return df, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start(subreddits, time, n_posts):\n",
    "    \n",
    "    # load data frame\n",
    "    df = load_dataframe()\n",
    "    ids = []\n",
    "    \n",
    "    # get all posts of all 15 subreddits\n",
    "    for sub in subreddits:\n",
    "        posts = reddit.subreddit(sub).new(limit=n_posts)\n",
    "        ids += [item.id for item in posts]\n",
    "        \n",
    "    # run the tracker\n",
    "    df = run(ids, time, df)\n",
    "    \n",
    "    # run the tagger\n",
    "    #df = tag_fake_news(df)\n",
    "    \n",
    "    # save result to .csv\n",
    "    df.to_csv(r'results.csv', index=False, header=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set subs to track\n",
    "subs = ['news', 'politics', 'worldnews', 'Uplifitingnews', 'coronavirus', 'covid19', \n",
    "        'worldevents', 'economics', 'environment', 'europe', 'republican', 'democrats', \n",
    "        'conservative', 'futurology', 'technology']\n",
    "\n",
    "# First argument is sub, second is number of hours, third is number of posts\n",
    "# df = start(subs, 12, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('Results folder\\\\results day 1 - 3.csv')\n",
    "# df = pd.concat([df, pd.read_csv('Results folder\\\\results day 4 - 6.csv')])\n",
    "# df = pd.concat([df, pd.read_csv('Results folder\\\\results day 7 - 9.csv')])\n",
    "# df = pd.concat([df, pd.read_csv('Results folder\\\\results day 10.csv')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files = ['Results folder\\\\results day 10.csv', 'Results folder\\\\results day 1 - 3.csv', \n",
    "         'Results folder\\\\results day 4 - 6.csv', 'Results folder\\\\results day 7 - 9.csv']\n",
    "\n",
    "for file in files:       \n",
    "    print(file)\n",
    "    df = pd.read_csv(file)\n",
    "    df = tag_fake_news(df, file)\n",
    "    df.to_csv(file, index=False, header=True)\n",
    "    clear_output()\n",
    "    \n",
    "for file in files:\n",
    "    print(file)\n",
    "    df = pd.read_csv(file)\n",
    "    df = tag_fake_news(df, file)\n",
    "    df.to_csv(file, index=False, header=True)\n",
    "    clear_output()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
